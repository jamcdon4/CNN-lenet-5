{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lenet5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOhcYYwusrx8",
        "colab_type": "code",
        "outputId": "1fa84a6c-67a7-4340-f00d-26fec84ebc5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxzMRpjN0mzD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "2a5b2051-24a4-489f-d8e3-f6029a7dd73d"
      },
      "source": [
        "## Methods and Imports\n",
        "\n",
        "import numpy as np\n",
        "from math import log\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() \n",
        "from tensorflow.keras.layers import Flatten\n",
        "\n",
        "# Read sample as either greyscale or color from path provided\n",
        "def get_samples(path):\n",
        "    samples = []\n",
        "    for file in os.listdir(path):\n",
        "        samples.append(cv2.imread(path + file, cv2.IMREAD_COLOR))\n",
        "    return samples\n",
        "\n",
        "# Increase size of image from 20x20 to 28x28 to match standard LeNet 5 artchitecture\n",
        "def upscale(data):\n",
        "    scale_percent = 140 # percent of original size\n",
        "    image_list = []\n",
        "    for img in data:\n",
        "        dst = cv2.resize(img, None, fx = 1.4, fy = 1.4, interpolation = cv2.INTER_CUBIC)\n",
        "        image_list.append(dst)\n",
        "    return image_list\n",
        "\n",
        "# Convert image array to grey scale\n",
        "def grayscale(data):\n",
        "    image_list = []\n",
        "    for img in data:\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        image_list.append(gray)\n",
        "    image_list = np.array(image_list)\n",
        "    image_list = np.reshape(image_list, (-1,32,32,1))\n",
        "    return np.array(image_list)\n",
        "\n",
        "# Provided validation/testing split and positive/negative samples, create arrays for train/test/valid labels and images\n",
        "def data_prep(valid_split,testing_split,positive_samples,negative_samples):\n",
        "\n",
        "    validation_split = valid_split * testing_split\n",
        "\n",
        "    # Point of split for validation and test, for both positive and negative samples\n",
        "    pos_split_test = int(len(positive_samples)*testing_split)\n",
        "    pos_split_valid = int(len(positive_samples)*validation_split)\n",
        "    neg_split_test = int(len(negative_samples)*testing_split)\n",
        "    neg_split_valid = int(len(negative_samples)*validation_split)\n",
        "\n",
        "    # Create the array to hold images in shape of [#samples, dim1, dim2, color channels]\n",
        "    training_set = positive_samples[0:pos_split_valid] + negative_samples[0:neg_split_valid]\n",
        "    validation_set = positive_samples[pos_split_valid:pos_split_test] + negative_samples[neg_split_valid:neg_split_test]\n",
        "    testing_set = positive_samples[pos_split_test:] + negative_samples[neg_split_test:]\n",
        "\n",
        "    # Add 0's and 1's for image labels \n",
        "    training_labels = [1] * len(positive_samples[0:pos_split_valid]) + [0] * len(negative_samples[0:neg_split_valid])\n",
        "    validation_labels = [1] * len(positive_samples[pos_split_valid:pos_split_test]) + [0] * len(negative_samples[neg_split_valid:neg_split_test])\n",
        "    testing_labels = [1] * len(positive_samples[pos_split_test:]) + [0] * len(negative_samples[neg_split_test:])\n",
        "\n",
        "    # Pad images with 0s\n",
        "    training_set      = np.pad(training_set, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
        "    validation_set    = np.pad(validation_set, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
        "    testing_set       = np.pad(testing_set, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
        "    return training_set,training_labels,validation_set,validation_labels,testing_set,testing_labels\n",
        "\n",
        "\n",
        "# build using tensorflow for LeNet 5 network\n",
        "def LeNet(x, activ_func):\n",
        "    mu = 0\n",
        "    sigma = 0.1\n",
        "    \n",
        "    # convolutional layer 1. input = 32x32x1. output = 28x28x6\n",
        "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5,5,1,6), mean=mu, stddev=sigma))\n",
        "    conv1_b = tf.Variable(tf.zeros(6))\n",
        "    conv1 = tf.nn.conv2d(x, conv1_W, strides=[1,1,1,1], padding='VALID') + conv1_b\n",
        "    \n",
        "    # activation with relu\n",
        "    conv1 = activ_func(conv1)\n",
        "    \n",
        "    # max pooling. input = 28x28x6. output = 14x14x6\n",
        "    conv1 = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
        "    \n",
        "    # convolutional layer 2. input = 14x14x6. output = 10x10x16\n",
        "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5,5,6,16), mean=mu, stddev=sigma))\n",
        "    conv2_b = tf.Variable(tf.zeros(16))\n",
        "    conv2 = tf.nn.conv2d(conv1, conv2_W, strides=[1,1,1,1], padding='VALID') + conv2_b\n",
        "    \n",
        "    # activation with relu\n",
        "    conv2 = activ_func(conv2)\n",
        "    \n",
        "    # max pooling. input = 10x10x16. output = 5x5x16\n",
        "    conv2 = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
        "    \n",
        "    # flatten. input = 10x10x6. output = 400\n",
        "    fc0   = Flatten()(conv2)\n",
        "    \n",
        "    # layer 3: fully connected layer. input = 400. output = 120\n",
        "    fc1_W = tf.Variable(tf.truncated_normal(shape=(400,120), mean=mu, stddev=sigma))\n",
        "    fc1_b = tf.Variable(tf.zeros(120))\n",
        "    fc1 = tf.matmul(fc0, fc1_W) + fc1_b\n",
        "    \n",
        "    # activation with relu\n",
        "    fc1 = activ_func(fc1)\n",
        "    \n",
        "    # drop out to prevent overfitting\n",
        "    fc1 = tf.nn.dropout(fc1, keep_probability)\n",
        "    \n",
        "    # layer 4: fully connected layer. input = 120. output = 84\n",
        "    fc2_W = tf.Variable(tf.truncated_normal(shape=(120,84), mean=mu, stddev=sigma))\n",
        "    fc2_b = tf.Variable(tf.zeros(84))\n",
        "    fc2 = tf.matmul(fc1, fc2_W) + fc2_b\n",
        "    \n",
        "    # activation with relu\n",
        "    fc2 = activ_func(fc2)\n",
        "    \n",
        "    fc2 = tf.nn.dropout(fc2, keep_probability)\n",
        "\n",
        "    # layer 5: fully connected layer. input = 84. output = 2\n",
        "    fc3_W = tf.Variable(tf.truncated_normal(shape=(84,2), mean=mu, stddev=sigma))\n",
        "    fc3_b = tf.Variable(tf.zeros(2))\n",
        "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
        "    \n",
        "    return logits\n",
        "\n",
        "def evaluate(X_data, y_data):\n",
        "    num_examples = len(X_data)\n",
        "    total_accuracy = 0\n",
        "    sess = tf.get_default_session()\n",
        "    for offset in range(0, num_examples, BATCH_SIZE):\n",
        "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
        "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, keep_probability: 1.0})\n",
        "        total_accuracy += (accuracy * len(batch_x))\n",
        "    return total_accuracy / num_examples"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sViHvawnuyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate samples from Dataset\n",
        "positive_samples = get_samples(\"/content/drive/My Drive/school/Machine Learning/ECE 763/positive_samples/\")\n",
        "negative_samples = get_samples(\"/content/drive/My Drive/school/Machine Learning/ECE 763/negative_samples/\")\n",
        "print('Original Image Dimensions : ',positive_samples[0].shape)\n",
        "\n",
        "# Upscale dimensions\n",
        "positive_samples = upscale(positive_samples)\n",
        "negative_samples = upscale(negative_samples)\n",
        "print('Rezied Image Dimensions : ',positive_samples[0].shape)\n",
        "print()\n",
        "\n",
        "print(\"Total samples before Data Augmentation: \")\n",
        "print(\"Positives: \" + str(len(positive_samples)))\n",
        "print(\"Negatives: \" + str(len(negative_samples)))\n",
        "\n",
        "# Data Augmentation - rotate negative images\n",
        "negative_samples_90 = [np.rot90(el) for el in negative_samples]\n",
        "negative_samples_180 = [np.rot90(el, 2) for el in negative_samples]\n",
        "negative_samples_270 = [np.rot90(el, 3) for el in negative_samples]\n",
        "negative_samples = negative_samples + negative_samples_90 + negative_samples_180 + negative_samples_270\n",
        "\n",
        "# Data Augmentation - horizontally flip positive images\n",
        "positive_samples_flip = [np.flip(el, 1) for el in positive_samples]\n",
        "positive_samples = positive_samples + positive_samples_flip\n",
        "\n",
        "print(\"Total samples after Data Augmentation: \")\n",
        "print(\"Positives: \" + str(len(positive_samples)))\n",
        "print(\"Negatives: \" + str(len(negative_samples)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-jCFXjqOpIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate arrays of labels and images for split data \n",
        "X_train,y_train,X_valid,y_valid,X_test,y_test = data_prep(0.80, 0.90,positive_samples,negative_samples)\n",
        "\n",
        "# Print info on split image arrays\n",
        "n_classes = np.unique(y_train).size\n",
        "print(\"Image Shape after padding: {}\".format(X_train[0].shape))\n",
        "print(\"Number of Classes: {}\".format(n_classes))\n",
        "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
        "print(\"Validation Set: {} samples\".format(len(X_valid)))\n",
        "print(\"Test Set:       {} samples\".format(len(X_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t7UglADTNwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Distribution of labels\n",
        "plt.hist(y_train, bins=n_classes, label='Train')\n",
        "plt.hist(y_valid, bins=n_classes, label='Validation')\n",
        "plt.hist(y_test, bins=n_classes, label='Test')\n",
        "plt.tick_params(axis='x')\n",
        "plt.tick_params(axis='y')\n",
        "plt.ylabel(\"Number of Samples\")\n",
        "plt.xlabel(\"Negative / Positive Class\")\n",
        "plt.legend()\n",
        "plt.title(\"Class Sample Split\")\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACxBv-STUKvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print an example of image\n",
        "\n",
        "index = random.randint(0, len(X_train))\n",
        "image = X_train[index]\n",
        "\n",
        "print('Image label:' + str(y_train[index]))\n",
        "plt.imshow(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f3o1MmTpgar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Printing Data Augmentation\n",
        "\n",
        "\n",
        "for test_var in [positive_samples,positive_samples_flip,negative_samples,negative_samples_90,negative_samples_180,negative_samples_270]:\n",
        "  image = test_var[0]\n",
        "  plt.figure()\n",
        "  plt.figure(figsize=(3,3))\n",
        "  plt.imshow(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xdQsRg6XcFn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# grascale data - CAN'T RERUN\n",
        "X_train = grayscale(X_train).astype('float')\n",
        "X_valid = grayscale(X_valid).astype('float')\n",
        "X_test = grayscale(X_test).astype('float')\n",
        "\n",
        "index = random.randint(0, len(X_train))\n",
        "image = X_train[index].squeeze()\n",
        "\n",
        "plt.figure(figsize=(1,1))\n",
        "plt.imshow(image, cmap=\"gray\")\n",
        "print('Image Label (greyscale): ' + str(y_train[index]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmIRY8S6amaI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normalize data\n",
        "X_train = (X_train - 128)/128 \n",
        "X_valid = (X_valid - 128)/128 \n",
        "X_test = (X_test - 128)/128 \n",
        "\n",
        "# shuffle data\n",
        "X_train, y_train = shuffle(X_train, y_train)\n",
        "X_valid, y_valid = shuffle(X_valid, y_valid)\n",
        "X_test, y_test = shuffle(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJTnJ5MYXaF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Training and Evaluation Pipeline\n",
        "\n",
        "best_valacc = np.zeros((100,), dtype=int)\n",
        "best_testacc = np.zeros((100,), dtype=int)\n",
        "# Best: [rate_var, dr_var, batch_var, activ_var]\n",
        "best_model = [0,0,0,0]\n",
        "\n",
        "for rate_var in [.001, .002, .003]:\n",
        "  for dr_var in [.1, .2, .3]:\n",
        "    for batch_var in [64, 128, 256]:\n",
        "      for activ_var in [tf.nn.relu,tf.nn.relu6,tf.nn.selu]:\n",
        "\n",
        "        # Save accuracy metrics for plotting and evaluation\n",
        "        save_valacc = []\n",
        "        save_testacc = []\n",
        "\n",
        "        # Hyperparameter testing\n",
        "\n",
        "        #[.001, .002, .003]\n",
        "        rate = rate_var\n",
        "        #[.1, .2, .3]\n",
        "        dropout_rate = dr_var\n",
        "        #[64, 128, 256]\n",
        "        BATCH_SIZE = batch_var\n",
        "        #[tf.nn.relu,tf.nn.relu6,tf.nn.selu]\n",
        "        activ_func = activ_var\n",
        "\n",
        "\n",
        "        # Regression Analysis\n",
        "        #['LASSO','RIDGE','NONE']\n",
        "        regression_type = 'RIDGE'\n",
        "\n",
        "        # Set high to measure overfitting\n",
        "        EPOCHS = 100\n",
        "\n",
        "        # Features and Labels for evaluation\n",
        "        x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
        "        y = tf.placeholder(tf.int32, (None))\n",
        "        keep_probability = tf.placeholder(tf.float32)\n",
        "        one_hot_y = tf.one_hot(y, 2)\n",
        "\n",
        "        # Create variables for linear regression\n",
        "        A = tf.Variable(tf.random_normal(shape=[1,1]))\n",
        "        b = tf.Variable(tf.random_normal(shape=[1,1]))\n",
        "\n",
        "        # Initialize model and entropy function\n",
        "        logits = LeNet(x, activ_func)\n",
        "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=one_hot_y)\n",
        "\n",
        "        # Select Regression Type\n",
        "        if regression_type == 'LASSO':\n",
        "            # Declare Lasso loss function\n",
        "            # Lasso Loss = L2_Loss + heavyside_step,\n",
        "            # Where heavyside_step ~ 0 if A < constant, otherwise ~ 99\n",
        "            lasso_param = tf.constant(0.9)\n",
        "            heavyside_step = tf.truediv(1., tf.add(1., tf.exp(tf.multiply(-50., tf.subtract(A, lasso_param)))))\n",
        "            regularization_param = tf.multiply(heavyside_step, 99.)\n",
        "            loss_operation = tf.add(tf.reduce_mean(cross_entropy), regularization_param)\n",
        "\n",
        "        elif regression_type == 'RIDGE':\n",
        "            # Declare the Ridge loss function\n",
        "            # Ridge loss = L2_loss + L2 norm of slope\n",
        "            ridge_param = tf.constant(1.)\n",
        "            ridge_loss = tf.reduce_mean(tf.square(A))\n",
        "            loss_operation = tf.expand_dims(tf.add(tf.reduce_mean(cross_entropy), tf.multiply(ridge_param, ridge_loss)), 0)\n",
        "            \n",
        "        else:\n",
        "            loss_operation = tf.reduce_mean(cross_entropy)\n",
        "\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
        "        training_operation = optimizer.minimize(loss_operation)\n",
        "\n",
        "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
        "        accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "        saver = tf.train.Saver()\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            num_examples = X_train.shape[0]\n",
        "            print(\"Training for:\")\n",
        "            print(\"Learning rate:\" + str(rate_var))\n",
        "            print(\"Dropout rate: \" + str(dr_var))\n",
        "            print(\"Batch size: \" + str(batch_var))\n",
        "            print(\"Activation function: \" + str(activ_var))\n",
        "            print()\n",
        "            for i in range(EPOCHS):\n",
        "                X_train, y_train = shuffle(X_train, y_train)\n",
        "                for offset in range(0, num_examples, BATCH_SIZE):\n",
        "                    end = offset + BATCH_SIZE\n",
        "                    batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
        "                    sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, keep_probability: 1-dropout_rate})\n",
        "\n",
        "                validation_accuracy = evaluate(X_valid, y_valid)\n",
        "                test_accuracy = evaluate(X_test, y_test)\n",
        "\n",
        "                save_valacc.append(validation_accuracy)\n",
        "                save_testacc.append(test_accuracy)\n",
        "\n",
        "            saver.save(sess, './lenet')\n",
        "            print(\"Test average for last 50 epochs: \" + str(np.mean(save_testacc[50:100])))\n",
        "            print(\"Model saved\")\n",
        "            print()\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
        "\n",
        "            test_accuracy = evaluate(X_test, y_test)\n",
        "            print(\"Final Test Accuracy = {:.3f}\".format(test_accuracy))\n",
        "\n",
        "        # Print graph for every run\n",
        "        t = np.arange(0, len(save_testacc), 1)\n",
        "        line1 = plt.plot(t, save_valacc, label='Validation Accuracy')\n",
        "        line2 = plt.plot(t, save_testacc, label='Testing Accuracy')\n",
        "        plt.tick_params(axis='x')\n",
        "        plt.tick_params(axis='y')\n",
        "        plt.legend(loc='lower right')\n",
        "        plt.ylabel(\"Accuracy\")\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.title(\"Evaluation of Model\")\n",
        "        plt.show()\n",
        "\n",
        "        # Save best model for plotting \n",
        "        if np.mean(save_testacc[50:100]) > np.mean(best_testacc[50:100]):\n",
        "          best_model = [rate_var, dr_var, batch_var, activ_var]\n",
        "          best_testacc = save_testacc\n",
        "          best_valacc = save_valacc\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-WDb5p8F61o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print graph for every run\n",
        "t = np.arange(0, len(best_testacc), 1)\n",
        "line1 = plt.plot(t, best_valacc, label='Validation Accuracy')\n",
        "line2 = plt.plot(t, best_testacc, label='Testing Accuracy')\n",
        "plt.tick_params(axis='x')\n",
        "plt.tick_params(axis='y')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.title(\"Evaluation of Best Model\")\n",
        "plt.show()\n",
        "\n",
        "print('Best model:')\n",
        "print('learning rate, dropout rate, batch size, activation function')\n",
        "print(best_model)\n",
        "print(\"Test average for last 50 epochs: \" + str(np.mean(best_testacc[50:100])))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}